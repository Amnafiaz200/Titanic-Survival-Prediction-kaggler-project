{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oo2zeBOETo_"
      },
      "source": [
        "# Titanic Survival Prediction — Custom Decision Tree and Random Forest\n",
        "\n",
        "In this project, we build two models **from scratch** (without using scikit-learn classifiers):\n",
        "\n",
        "1. **Part 1 — Custom Decision Tree**\n",
        "2. **Part 2 — Custom Random Forest**\n",
        "2. **Part 3 — SVM**\n",
        "\n",
        "We train them on the **Titanic dataset** to predict passenger survival.  \n",
        "We also analyze model accuracy and compare results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKZHD9zAE-vT"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSVAUX31DEHv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import Dict, Any\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Optional, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2qF5hbFCkH"
      },
      "source": [
        "### **Part 1 — Custom Decision Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaT38A6eHkPI"
      },
      "source": [
        "### Load and Prepare Titanic Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3C_hGPMFRSS",
        "outputId": "bb9342c7-6994-44a7-b67b-3d7cb7c72ea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n",
            "None\n",
            "PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load training data\n",
        "data = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "print(data.head())\n",
        "print(data.info())\n",
        "print(data.isnull().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2BxHqr8QCRL"
      },
      "source": [
        "The Titanic dataset contains passenger information such as age, class, gender, fare, and survival status. Some columns contain missing values and categorical data that need to be handled properly.\n",
        "\n",
        "---\n",
        "\n",
        "## **Task: Implement Data Preprocessing Functions**\n",
        "\n",
        "Its is required to write Python functions (without using advanced preprocessing libraries like `sklearn.preprocessing`) to perform the following steps manually.\n",
        "\n",
        "---\n",
        "\n",
        "### *Step 1: Handle Missing Values*\n",
        "\n",
        "Write functions to:\n",
        "\n",
        "1. Replace missing values in the **Age** column with the **median age** of all passengers.  \n",
        "2. Replace missing values in the **Embarked** column with the **most common embarkation port (mode)**.\n",
        "\n",
        "---\n",
        "\n",
        "### *Step 2: Drop Irrelevant Columns*\n",
        "\n",
        "Remove the following columns that are not directly useful for survival prediction:\n",
        "\n",
        "1. **Cabin** – too many missing values  \n",
        "2. **Ticket** – non-numeric and not informative  \n",
        "3. **Name** – mostly unique values  \n",
        "4. **PassengerId** – identifier, not a feature\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Encode Categorical Variables**\n",
        "\n",
        "Since machine learning models require numerical data, we need to convert categorical variables into numeric format manually.\n",
        "\n",
        "#### Encode the `Sex` column:\n",
        "- `male` → `0`  \n",
        "- `female` → `1`\n",
        "\n",
        "#### Encode the `Embarked` column:\n",
        "- `C` → `0`  \n",
        "- `Q` → `1`  \n",
        "- `S` → `2`\n",
        "\n",
        "This manual encoding ensures that categorical features can be interpreted correctly by machine learning algorithms, which typically work only with numerical inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGRBIYK1PoM8"
      },
      "outputs": [],
      "source": [
        "# Handle missing value\n",
        "def impute_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Handles missing values in the Age and Embarked columns.\n",
        "    - Replaces missing 'Age' values with the median of the column.\n",
        "    - Replaces missing 'Embarked' values with the mode.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Step 1: Handle Missing Values by imputation ---\")\n",
        "\n",
        "    # Impute Age with Median, The median is robust to outliers, making it a good choice for continuous data.\n",
        "    median_age = df['Age'].median()\n",
        "    df['Age'].fillna(median_age, inplace=True)\n",
        "    print(f\"Filled missing Age values with the median: {median_age:.2f}\")\n",
        "\n",
        "    # Impute Embarked with Mode, The mode is the most appropriate central tendency measure for categorical data.\n",
        "    mode_embarked = df['Embarked'].mode().iloc[0]        # .iloc[0] is used because mode() can return multiple modes if they share the same count.\n",
        "    df['Embarked'].fillna(mode_embarked, inplace=True)\n",
        "    print(f\"Filled missing Embarked values with the mode: {mode_embarked}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Drop unused columns\n",
        "def drop_columns(df: pd.DataFrame, columns_to_drop: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes columns that are considered irrelevant for survival prediction.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Step 2: Drop Irrelevant Columns ---\")\n",
        "    df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
        "    print(f\"Dropped columns: {columns_to_drop}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Encode categorical features manually\n",
        "def encode_categoricals(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Manually converts specified categorical columns into numerical format.\n",
        "    - Sex: male -> 0, female -> 1\n",
        "    - Embarked: C -> 0, Q -> 1, S -> 2\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Step 3: Encode Categorical Variables ---\")\n",
        "\n",
        "    # Encode 'Sex'\n",
        "    sex_mapping = {'male': 0, 'female': 1}\n",
        "    df['Sex'] = df['Sex'].replace(sex_mapping)\n",
        "    print(f\"Encoded 'Sex'column using mapping: {sex_mapping}\")\n",
        "\n",
        "    # Encode 'Embarked'\n",
        "    embarked_mapping = {'C': 0, 'Q': 1, 'S': 2}\n",
        "    df['Embarked'] = df['Embarked'].replace(embarked_mapping)\n",
        "    print(f\"Encoded 'Embarked'column using mapping: {embarked_mapping}\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0gKVyfFPsL5"
      },
      "source": [
        "Split data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x88W8Z3NPqos"
      },
      "outputs": [],
      "source": [
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbBTH304Hrsd"
      },
      "source": [
        "## **Task: Implement Core Functions for Decision Tree Splitting**\n",
        "\n",
        "In this section, we will implement the key mathematical components that allow a **Decision Tree** to decide where to split data during training. we will write three main functions: `entropy`, `information_gain`, and `best_split`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Compute Entropy**\n",
        "- The entropy function measures the impurity or uncertainty in a dataset.  \n",
        "- Formula:  \n",
        "  $$\n",
        "  H(y) = -\\sum_i p_i \\log_2(p_i)\n",
        "  $$\n",
        "  where $p_i$ is the probability of each class label.\n",
        "- Use NumPy operations to calculate probabilities.\n",
        "- Add a small constant (e.g., `1e-9`) inside the log to avoid numerical errors.\n",
        "- Test on simple arrays, e.g., `entropy([0, 0, 1, 1])`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Compute Information Gain**\n",
        "- Information Gain (IG) quantifies how much entropy decreases after a split.\n",
        "- Formula:  \n",
        "  $$\n",
        "  IG = H(\\text{parent}) - \\frac{n_{left}}{n}H(\\text{left}) - \\frac{n_{right}}{n}H(\\text{right})\n",
        "  $$\n",
        "- Implement this using the `entropy()` function.\n",
        "- The higher the IG, the better the feature/threshold for splitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Find the Best Split**\n",
        "- The `best_split()` function finds which feature and threshold produce the **maximum information gain**.\n",
        "- Loop through all features (or a given subset) and:\n",
        "  - For **numerical features**:\n",
        "    - Try splitting at all unique threshold values.\n",
        "    - Create boolean masks for left (`<= t`) and right (`> t`) splits.\n",
        "  - For **categorical features**:\n",
        "    - Split data based on equality (`== val`) vs. inequality (`!= val`).\n",
        "- Skip invalid splits (e.g., when one side is empty).\n",
        "- Return:\n",
        "  - `best_feature`: the most informative feature,\n",
        "  - `best_threshold`: the split value,\n",
        "  - `best_gain`: the highest information gain achieved.\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output**\n",
        "- `entropy()` returns 0 when all samples belong to one class.\n",
        "- `information_gain()` returns higher values for better splits.\n",
        "- `best_split()` identifies the optimal feature and threshold for partitioning the data.\n",
        "\n",
        "Implement the functions below step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o766sg4YHsvI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Tuple, Optional, Union, Dict, Any, List  #list and dict are used in the next cell\n",
        "\n",
        "# A small constant to prevent log(0)\n",
        "EPSILON = 1e-9\n",
        "\n",
        "def entropy(y):\n",
        "    \"\"\"\n",
        "    Compute the entropy of a label array y.\n",
        "    Formula: H(y) = -Σ p_i * log2(p_i)\n",
        "    \"\"\"\n",
        "    if len(y) == 0:\n",
        "      return 0.0\n",
        "\n",
        "    # Get the count of unique classes\n",
        "    class_labels, counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    # Calculate probabilities p_i\n",
        "    probabilities = counts / len(y)\n",
        "    #Formula\n",
        "    entropy_value = -np.sum(probabilities * np.log2(probabilities + EPSILON))\n",
        "\n",
        "    return entropy_value\n",
        "\n",
        "\n",
        "def information_gain(y, y_left, y_right):\n",
        "    \"\"\"\n",
        "    Compute the information gain of a split.\n",
        "    IG = H(parent) - (n_left/n)*H(left) - (n_right/n)*H(right)\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "    n_left = len(y_left)\n",
        "    n_right = len(y_right)\n",
        "\n",
        "    if n_left == 0 or n_right == 0:\n",
        "        # Avoid non-splits\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate entropy for the parent node\n",
        "    parent_entropy = entropy(y)\n",
        "\n",
        "    # Calculate weighted entropy for the child nodes\n",
        "    weighted_child_entropy = (n_left / n) * entropy(y_left) + \\\n",
        "                             (n_right / n) * entropy(y_right)\n",
        "\n",
        "    # Calculate Information Gain\n",
        "    ig = parent_entropy - weighted_child_entropy\n",
        "\n",
        "    return ig\n",
        "\n",
        "\n",
        "def best_split(X, y, feature_subset=None):\n",
        "    \"\"\"\n",
        "    Find the best feature and threshold that maximize information gain.\n",
        "\n",
        "    Parameters:\n",
        "        X (DataFrame): Feature dataset\n",
        "        y (Series or array): Target labels\n",
        "        feature_subset (list): Optional subset of features for random forest\n",
        "\n",
        "    Returns:\n",
        "        best_feature (str): Feature name with best split\n",
        "        best_threshold (float/str): Threshold or category value\n",
        "        best_gain (float): Information gain of best split\n",
        "    \"\"\"\n",
        "    best_gain = -1.0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "\n",
        "    features = feature_subset if feature_subset is not None else X.columns\n",
        "\n",
        "    for feature_name in features:\n",
        "        feature_data = X[feature_name]\n",
        "\n",
        "        # Sort unique values\n",
        "        unique_thresholds = np.sort(feature_data.unique())\n",
        "\n",
        "        for threshold in unique_thresholds:\n",
        "            # 1. Create split masks for numerical features\n",
        "            mask_left = feature_data <= threshold\n",
        "            mask_right = feature_data > threshold\n",
        "\n",
        "            # Check if split is valid (both sides must have samples)\n",
        "            if np.sum(mask_left) == 0 or np.sum(mask_right) == 0:\n",
        "                continue\n",
        "\n",
        "            # 2. Split the target array y\n",
        "            y_left = y[mask_left]\n",
        "            y_right = y[mask_right]\n",
        "\n",
        "            # 3. Calculate Information Gain\n",
        "            current_gain = information_gain(y, y_left, y_right)\n",
        "\n",
        "            # 4. Update best split if current gain is higher\n",
        "            if current_gain > best_gain:\n",
        "                best_gain = current_gain\n",
        "                best_feature = feature_name\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature, best_threshold, best_gain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndw7zuM3H6np"
      },
      "source": [
        "## **Task: Implement the Custom Decision Tree Classifier**\n",
        "\n",
        "In this task, we will implement your own **Decision Tree** algorithm from scratch — without using libraries like `sklearn.tree`.  \n",
        "This implementation will rely on the previously defined helper functions: `entropy`, `information_gain`, and `best_split`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Class Overview**\n",
        "\n",
        "we will create a `DecisionTree` class with the following methods:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. `__init__()`**\n",
        "- Initializes the tree’s hyperparameters:\n",
        "  - `max_depth`: the maximum allowed depth of the tree.  \n",
        "  - `min_samples_split`: minimum number of samples required to make a split.  \n",
        "  - `feature_subsample`: number of randomly selected features (used in Random Forests).  \n",
        "- The tree itself will be stored as a nested dictionary (`self.tree`).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `fit()`**\n",
        "- Recursively builds the decision tree using **Information Gain** as the splitting criterion.\n",
        "- **Base cases**:\n",
        "  - If there are no samples left (`len(y) == 0`) → return `0`.\n",
        "  - If all samples belong to the same class → return that class label.\n",
        "  - If maximum depth is reached or too few samples remain → return the most common class (majority vote).\n",
        "- **Recursive case**:\n",
        "  1. Select the best feature and threshold using `best_split()`.\n",
        "  2. Split the dataset into left and right subsets based on that threshold.\n",
        "  3. Recursively call `fit()` on each subset to build subtrees.\n",
        "- The final tree should be stored in `self.tree` as nested dictionaries with keys:\n",
        "..., 'threshold': ..., 'gain': ..., 'left': ..., 'right': ...}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **3. `predict_one()`**\n",
        "- Predict the class for a **single data point**.\n",
        "- Traverse the decision tree recursively:\n",
        "- For numeric thresholds: use `<=` for the left branch, `>` for the right.\n",
        "- For categorical values: use `==` for the left branch, `!=` for the right.\n",
        "- Stop recursion when a leaf node (class label) is reached.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. `predict()`**\n",
        "- Predict the class for **an entire dataset** (`X`).\n",
        "- Apply `predict_one()` to each row using a loop or list comprehension.\n",
        "- Return predictions as a NumPy array.\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Behavior**\n",
        "- The `fit()` method should build a decision tree automatically.\n",
        "- The `predict()` method should output the correct class predictions for given input samples.\n",
        "- You should be able to check accuracy by comparing predicted and actual labels.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Usage (after implementation)**\n",
        "\n",
        "```python\n",
        "tree = DecisionTree(max_depth=4)\n",
        "tree.fit(X_train, y_train)\n",
        "predictions = tree.predict(X_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E-Ya-TkH7ON"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, feature_subsample=None):\n",
        "        \"\"\"\n",
        "        Initialize the Decision Tree parameters.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.feature_subsample = feature_subsample\n",
        "        self.tree: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    def _majority_vote(self, y: np.ndarray) -> int:\n",
        "        \"\"\"Returns the most common class label in array y.\"\"\"\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        # Counter().most_common(1) returns [ (label, count) ]\n",
        "        return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "    def _get_feature_subset(self, all_features: pd.Index) -> List[str]:\n",
        "        \"\"\"Selects a random subset of features.\"\"\"\n",
        "        if self.feature_subsample is None or self.feature_subsample >= len(all_features):\n",
        "            return all_features.tolist()\n",
        "\n",
        "        return np.random.choice(all_features, size=self.feature_subsample, replace=False).tolist()\n",
        "\n",
        "\n",
        "    def fit(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively build the decision tree using information gain.\n",
        "        \"\"\"\n",
        "        #Base Cases:\n",
        "\n",
        "        # Base Case 1:\n",
        "        if len(np.unique(y)) == 1:\n",
        "            return y[0]\n",
        "\n",
        "        # Base Case 2: Max Depth\n",
        "        if depth >= self.max_depth or len(y) < self.min_samples_split:\n",
        "            return self._majority_vote(y)\n",
        "\n",
        "        # Ensure relevant columns are numeric before finding best split\n",
        "        # This is a safeguard in case preprocessing wasn't fully applied or consistent\n",
        "        X_processed = X.copy()\n",
        "        for col in ['Age', 'Fare', 'Pclass', 'SibSp', 'Parch', 'Sex', 'Embarked']:\n",
        "             if col in X_processed.columns:\n",
        "                # Attempt conversion to numeric, coercing errors to NaN and then handling NaNs if any remain\n",
        "                X_processed[col] = pd.to_numeric(X_processed[col], errors='coerce')\n",
        "\n",
        "\n",
        "        # Recursive Case: Find Best Split\n",
        "        features = self._get_feature_subset(X_processed.columns)\n",
        "        feature, threshold, gain = best_split(X_processed, y, feature_subset=features)\n",
        "\n",
        "        # Base Case 3:\n",
        "        if feature is None or gain <= EPSILON:\n",
        "            return self._majority_vote(y)\n",
        "\n",
        "        # 1. Split Data\n",
        "        # Use the processed data for splitting\n",
        "        mask_left = X_processed[feature] <= threshold\n",
        "        X_left, y_left = X_processed[mask_left], y[mask_left]\n",
        "        X_right, y_right = X_processed[mask_left], y[mask_left]\n",
        "\n",
        "\n",
        "        # 2. Recursively build subtrees\n",
        "        left_subtree = self.fit(X_left, y_left, depth + 1)\n",
        "        right_subtree = self.fit(X_right, y_right, depth + 1)\n",
        "\n",
        "        # 3. Build Node Dictionary\n",
        "        node = {\n",
        "            'feature': feature,\n",
        "            'threshold': threshold,\n",
        "            'gain': gain,\n",
        "            'left': left_subtree,\n",
        "            'right': right_subtree\n",
        "        }\n",
        "\n",
        "        # Store the tree structure only at the root call\n",
        "        if depth == 0:\n",
        "            self.tree = node\n",
        "            return self\n",
        "        else:\n",
        "            return node\n",
        "\n",
        "\n",
        "    def predict_one(self, x, node=None):\n",
        "        \"\"\"\n",
        "        Predict class label for a single sample.\n",
        "        \"\"\"\n",
        "        if node is None:\n",
        "            node = self.tree\n",
        "            if node is None:\n",
        "                raise ValueError(\"Tree not fitted. Call the function fit() first.\")\n",
        "\n",
        "        if not isinstance(node, dict):  # If the node is not a dictionary, it's a leaf node (class label)\n",
        "            return node\n",
        "\n",
        "        feature = node['feature']\n",
        "        threshold = node['threshold']\n",
        "\n",
        "        # to ensure the feature value is numeric for comparison if the threshold is numeric\n",
        "        value = x[feature]\n",
        "        if isinstance(threshold, (int, float)):\n",
        "             # Attempt conversion to numeric, coercing errors to NaN\n",
        "             value = pd.to_numeric(value, errors='coerce')\n",
        "\n",
        "             if pd.isna(value):\n",
        "                 return self.predict_one(x, node['right'])\n",
        "\n",
        "        #Numerical split\n",
        "        if isinstance(threshold, (int, float)):\n",
        "            if value <= threshold:\n",
        "                return self.predict_one(x, node['left'])\n",
        "            else:\n",
        "                return self.predict_one(x, node['right'])\n",
        "        # Categorical split (equality-based)\n",
        "        else:\n",
        "             if value == threshold:\n",
        "                 return self.predict_one(x, node['left'])\n",
        "             else:\n",
        "                 return self.predict_one(x, node['right'])\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for all rows in dataset X.\n",
        "        \"\"\"\n",
        "        if self.tree is None:\n",
        "            # Check if the tree was actually fitted\n",
        "            raise ValueError(\"Tree not fitted. Call fit() first.\")\n",
        "\n",
        "        # Apply predict_one to each row of the input DataFrame X\n",
        "        predictions = np.array([self.predict_one(X.iloc[i]) for i in range(len(X))])\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPgrt6_cIBOD"
      },
      "source": [
        "### **Train and Evaluate the Custom Decision Tree**\n",
        "\n",
        "It will be used to train, test, and evaluate your implemented `DecisionTree` class automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G1cADOJRIB3N",
        "outputId": "d32882e5-df4e-4e83-b668-a250679b1e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Step 1: Handle Missing Values by imputation ---\n",
            "Filled missing Age values with the median: 28.00\n",
            "Filled missing Embarked values with the mode: S\n",
            "\n",
            "--- Step 2: Drop Irrelevant Columns ---\n",
            "Dropped columns: ['Cabin', 'Ticket', 'Name', 'PassengerId']\n",
            "\n",
            "--- Step 3: Encode Categorical Variables ---\n",
            "Encoded 'Sex'column using mapping: {'male': 0, 'female': 1}\n",
            "Encoded 'Embarked'column using mapping: {'C': 0, 'Q': 1, 'S': 2}\n",
            "\n",
            "--- Step 1: Handle Missing Values by imputation ---\n",
            "Filled missing Age values with the median: 29.00\n",
            "Filled missing Embarked values with the mode: S\n",
            "\n",
            "--- Step 2: Drop Irrelevant Columns ---\n",
            "Dropped columns: ['Cabin', 'Ticket', 'Name', 'PassengerId']\n",
            "\n",
            "--- Step 3: Encode Categorical Variables ---\n",
            "Encoded 'Sex'column using mapping: {'male': 0, 'female': 1}\n",
            "Encoded 'Embarked'column using mapping: {'C': 0, 'Q': 1, 'S': 2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1697026587.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(median_age, inplace=True)\n",
            "/tmp/ipython-input-1697026587.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(mode_embarked, inplace=True)\n",
            "/tmp/ipython-input-1697026587.py:47: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['Sex'] = df['Sex'].replace(sex_mapping)\n",
            "/tmp/ipython-input-1697026587.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['Embarked'] = df['Embarked'].replace(embarked_mapping)\n",
            "/tmp/ipython-input-1697026587.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(median_age, inplace=True)\n",
            "/tmp/ipython-input-1697026587.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(mode_embarked, inplace=True)\n",
            "/tmp/ipython-input-1697026587.py:47: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['Sex'] = df['Sex'].replace(sex_mapping)\n",
            "/tmp/ipython-input-1697026587.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['Embarked'] = df['Embarked'].replace(embarked_mapping)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Decision Tree Accuracy: 0.4134\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"compare_tree\",\n  \"rows\": 179,\n  \"fields\": [\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "compare_tree"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-5c3d4ba2-9050-4038-8ae6-a8c008b1b337\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Actual</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c3d4ba2-9050-4038-8ae6-a8c008b1b337')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5c3d4ba2-9050-4038-8ae6-a8c008b1b337 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5c3d4ba2-9050-4038-8ae6-a8c008b1b337');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-14daba5e-13a1-4684-ba7b-5cf99394b171\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-14daba5e-13a1-4684-ba7b-5cf99394b171')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-14daba5e-13a1-4684-ba7b-5cf99394b171 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Actual  Predicted\n",
              "0       1          1\n",
              "1       0          1\n",
              "2       0          1\n",
              "3       1          1\n",
              "4       1          1\n",
              "5       1          1\n",
              "6       1          1\n",
              "7       0          1\n",
              "8       1          1\n",
              "9       1          1"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply preprocessing steps to X_train and X_test\n",
        "X_train_processed = impute_missing_values(X_train.copy())\n",
        "X_train_processed = drop_columns(X_train_processed, ['Cabin', 'Ticket', 'Name', 'PassengerId'])\n",
        "X_train_processed = encode_categoricals(X_train_processed)\n",
        "\n",
        "X_test_processed = impute_missing_values(X_test.copy())\n",
        "X_test_processed = drop_columns(X_test_processed, ['Cabin', 'Ticket', 'Name', 'PassengerId'])\n",
        "X_test_processed = encode_categoricals(X_test_processed)\n",
        "\n",
        "tree = DecisionTree(max_depth=4)\n",
        "tree.fit(X_train_processed, y_train)\n",
        "y_pred_tree = tree.predict(X_test_processed)\n",
        "\n",
        "acc_tree = np.mean(y_pred_tree == y_test)\n",
        "print(f\"Custom Decision Tree Accuracy: {acc_tree:.4f}\")\n",
        "\n",
        "compare_tree = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_tree})\n",
        "compare_tree.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-j2O3E5IF4v"
      },
      "source": [
        "## **Part 2 — Custom Random Forest**\n",
        "\n",
        "In this section, we will implement your own **Random Forest Classifier** from scratch — without using libraries like `sklearn.ensemble`.\n",
        "\n",
        "A Random Forest combines multiple Decision Trees to form an ensemble model that improves prediction accuracy and reduces overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Instructions**\n",
        "\n",
        "1. **Initialize Parameters (`__init__`)**\n",
        "   - `n_estimators`: Number of decision trees to train.\n",
        "   - `max_depth`: Maximum depth of each decision tree.\n",
        "   - `min_samples_split`: Minimum number of samples required to split a node.\n",
        "   - `feature_subsample_ratio`: Fraction of features to randomly select for each tree (e.g., 0.7 = 70% of features per tree).\n",
        "\n",
        "   Store all trees in a list called `self.trees`.\n",
        "\n",
        "---\n",
        "\n",
        "2. **Train the Model (`fit`)**\n",
        "   - For each tree:\n",
        "     - Create a **bootstrap sample** by randomly selecting rows from the dataset **with replacement**.\n",
        "     - Randomly choose a subset of features using the ratio `feature_subsample_ratio`.\n",
        "     - Train a new instance of your `DecisionTree` class on the sampled data.\n",
        "   - Append each trained tree to the list `self.trees`.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Make Predictions (`predict`)**\n",
        "   - Each trained decision tree makes predictions for all samples in `X`.\n",
        "   - Combine predictions from all trees using **majority voting** (most common class).\n",
        "   - Return the final predicted labels as a NumPy array.\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Behavior**\n",
        "- The Random Forest should generally achieve **equal or better accuracy** than your single Decision Tree.\n",
        "- The `fit()` and `predict()` methods should work seamlessly with your previously defined `DecisionTree` implementation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Goal**\n",
        "By completing this part, we will understand how ensemble learning improves model robustness through randomness and aggregation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Vc8WHCIHrT"
      },
      "source": [
        "Implement Custom Random Forest Class\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J8j0LjdIJil"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=5, min_samples_split=2, feature_subsample_ratio=0.7):\n",
        "        \"\"\"\n",
        "        Initialize the Random Forest parameters.\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.feature_subsample_ratio = feature_subsample_ratio\n",
        "        self.trees: List[DecisionTree] = []\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train multiple Decision Trees using bootstrapped samples and random feature subsets.\n",
        "        \"\"\"\n",
        "        n_samples = len(X)\n",
        "        n_features = len(X.columns)\n",
        "\n",
        "        # Calculate the actual number of features to subsample\n",
        "        feature_subsample_count = int(n_features * self.feature_subsample_ratio)\n",
        "\n",
        "        self.trees = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # 1. Create a bootstrap sample (sampling rows with replacement)\n",
        "            bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "            # Use iloc and .copy() for reliable sampling and type conversion prevention\n",
        "            X_sample = X.iloc[bootstrap_indices].copy()\n",
        "            y_sample = y[bootstrap_indices]\n",
        "\n",
        "            # 2. Train a new DecisionTree instance\n",
        "            tree = DecisionTree(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                feature_subsample=feature_subsample_count\n",
        "            )\n",
        "\n",
        "            tree.fit(X_sample, y_sample)\n",
        "\n",
        "            # 3. Append the trained tree\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for all samples using majority voting from all trained trees.\n",
        "        \"\"\"\n",
        "        if not self.trees:\n",
        "            raise ValueError(\"Random Forest not fitted. Call fit() first.\")\n",
        "\n",
        "        # Get predictions from all individual trees (Shape: n_estimators x n_samples)\n",
        "        predictions_all = np.array([tree.predict(X) for tree in self.trees])\n",
        "\n",
        "        # Combine predictions using majority voting across all trees for each sample\n",
        "        final_predictions = np.array([\n",
        "            Counter(predictions_all[:, i]).most_common(1)[0][0]\n",
        "            for i in range(predictions_all.shape[1])\n",
        "        ])\n",
        "\n",
        "        return final_predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFg7pCmIN4b"
      },
      "source": [
        "### **Train and Evaluate the Custom Random Forest**\n",
        "\n",
        "Do **not** change the following cell.  \n",
        "It will be used to train, test, and evaluate your implemented `RandomForest` class automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "0I0uPgZ6IOsi",
        "outputId": "e73c9fa3-db11-41dc-ce97-72163bf3095e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Random Forest Accuracy: 0.7318\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"compare_forest\",\n  \"rows\": 179,\n  \"fields\": [\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tree_Pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Forest_Pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "compare_forest"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ea64c888-8abb-4cfe-b845-edd599ed9e40\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Actual</th>\n",
              "      <th>Tree_Pred</th>\n",
              "      <th>Forest_Pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea64c888-8abb-4cfe-b845-edd599ed9e40')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ea64c888-8abb-4cfe-b845-edd599ed9e40 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ea64c888-8abb-4cfe-b845-edd599ed9e40');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0c83898c-e73a-4324-b132-ce925e54416c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c83898c-e73a-4324-b132-ce925e54416c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0c83898c-e73a-4324-b132-ce925e54416c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Actual  Tree_Pred  Forest_Pred\n",
              "0       1          1            0\n",
              "1       0          1            0\n",
              "2       0          1            0\n",
              "3       1          1            1\n",
              "4       1          1            1\n",
              "5       1          1            1\n",
              "6       1          1            1"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "forest = RandomForest(n_estimators=10, max_depth=5)\n",
        "forest.fit(X_train_processed, y_train)\n",
        "y_pred_forest = forest.predict(X_test_processed)\n",
        "\n",
        "acc_forest = np.mean(y_pred_forest == y_test)\n",
        "print(f\"Custom Random Forest Accuracy: {acc_forest:.4f}\")\n",
        "\n",
        "compare_forest = pd.DataFrame({\n",
        "    'Actual': y_test,\n",
        "    'Tree_Pred': y_pred_tree,\n",
        "    'Forest_Pred': y_pred_forest\n",
        "})\n",
        "compare_forest.head(7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67KbLuKrISal"
      },
      "source": [
        "### **Comparison**\n",
        "\n",
        "It compares the performance of your **Custom Decision Tree** and **Custom Random Forest** implementations by printing their respective accuracies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTar0OLyITkg",
        "outputId": "aa48562c-fca6-4d82-bbba-f20be0055ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree Accuracy: 0.4134\n",
            "Random Forest Accuracy: 0.7318\n"
          ]
        }
      ],
      "source": [
        "print(f\"Decision Tree Accuracy: {acc_tree:.4f}\")\n",
        "print(f\"Random Forest Accuracy: {acc_forest:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ovdCkSOTUC1"
      },
      "source": [
        "**Analytical Question:**  \n",
        "Based on the accuracy results of the Decision Tree and Random Forest models, explain **why the Random Forest might perform better or worse** than a single Decision Tree.  \n",
        "Discuss the roles of **ensemble learning**, **variance reduction**, and **overfitting** in your explanation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-YX7wNYTViY"
      },
      "source": [
        "**Answer:**\n",
        "Random Forest performed better than a single Decision Tree.This large improvement is due to the power of ensemble learning and variance reduction(high to low variance).\n",
        "\n",
        "A Random Forest combines predictions from many independent n Decision Trees (each trained on different n subsets of data and features).\n",
        "This making of multiple Decision Trees called ensemble learning which leverages the idea that while each tree may be imperfect, their combined “vote” tends to be much more accurate(Majority wins voting)\n",
        "\n",
        "A single Decision Tree is a high-variance model, a small changes in the training data can lead to very different outcomes.\n",
        "By using multiple trees (each trained on a random sample), Random Forest reduces variance without significantly increasing bias.\n",
        "\n",
        "A single Decision Tree tends to overfit training data because it grows deep and memorizes patterns specific to the dataset.\n",
        "Random Forest prevents overfitting by Bootstrap sampling (each tree sees a different subset of the data). The second one is random feature sampling, each tree uses a random subset of features at each split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVLhnD0O9_24"
      },
      "source": [
        "---\n",
        "## **Part 3 — Support Vector Machines (SVM) with Scikit-Learn**\n",
        "\n",
        "In this final part, we will explore another powerful supervised learning algorithm: the **Support Vector Machine (SVM)**. Unlike Decision Trees, which make splits by partitioning data, an SVM's goal is to find the optimal hyperplane or decision boundary that best separates the classes.\n",
        "\n",
        "we will use `scikit-learn`'s implementation to train an SVM on the same preprocessed Titanic data and compare its performance to your from-scratch models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNSTyvqf9_24"
      },
      "source": [
        "### **Task: Train and Evaluate SVM Classifiers**\n",
        "\n",
        "You will use `scikit-learn`'s `SVC` (Support Vector Classifier). We will experiment with two key hyperparameters: the `kernel` and the regularization parameter `C`.\n",
        "\n",
        "#### **Understanding SVM Kernels**\n",
        "The **kernel** allows the SVM to find complex, non-linear decision boundaries. We will use two types:\n",
        "* **`linear` kernel:** This attempts to separate the data with a single straight line (or a flat plane in higher dimensions). It's fast and works well if the data is linearly separable. Think of it as using a ruler to divide two groups of points.\n",
        "* **`rbf` kernel (Radial Basis Function):** This is the default and more flexible kernel. It can create complex, curved decision boundaries by mapping the data to a higher-dimensional space. Think of it as using a flexible wire to enclose different groups of points.\n",
        "\n",
        "#### **Understanding the Regularization Parameter `C`**\n",
        "The **`C` parameter** controls the trade-off between achieving a low training error and a low testing error.\n",
        "* A **low `C`** makes the decision boundary smoother and the margin wider. The model is more tolerant of misclassifying a few training points, which can lead to better **generalization** (less overfitting).\n",
        "* A **high `C`** aims to classify every training example correctly, which can lead to a more complex boundary and a narrower margin. This might **overfit** the training data.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Use the same `X_train`, `y_train`, `X_test`, and `y_test` data from Part 1.\n",
        "2.  Train one `SVC` model with a `linear` kernel as a baseline.\n",
        "3.  Train three separate `SVC` models with an `rbf` kernel, using `C` values of `0.1`, `1`, and `10`.\n",
        "4.  For each of the four models, calculate and print its accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juW8FmJbTZQs",
        "outputId": "abf56861-34e8-4134-de4e-9f4a01f1e131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM (Linear Kernel) Accuracy: 0.7821\n",
            "SVM (RBF Kernel, C=0.1) Accuracy: 0.6536\n",
            "SVM (RBF Kernel, C=1) Accuracy: 0.6592\n",
            "SVM (RBF Kernel, C=10) Accuracy: 0.7095\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Train and evaluate an SVM with a linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train_processed, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test_processed)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"SVM (Linear Kernel) Accuracy: {acc_linear:.4f}\")\n",
        "\n",
        "\n",
        "# 2. Train and evaluate SVMs with an RBF kernel and different C values\n",
        "rbf_accuracies = {}  # <-- Define this dictionary\n",
        "for C_value in [0.1, 1, 10]:\n",
        "    svm_rbf = SVC(kernel='rbf', C=C_value, random_state=42)\n",
        "    svm_rbf.fit(X_train_processed, y_train)\n",
        "    y_pred_rbf = svm_rbf.predict(X_test_processed)\n",
        "    acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "    rbf_accuracies[C_value] = acc_rbf  # <-- Store the accuracy in the dictionary\n",
        "    print(f\"SVM (RBF Kernel, C={C_value}) Accuracy: {acc_rbf:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LglkMKkl9_24"
      },
      "source": [
        "### **Final Comparison and Analysis**\n",
        "\n",
        "Now, let's compare the performance of all the models you've worked with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR1nFYEw9_24",
        "outputId": "f6a648ad-6783-4ce9-de99-538109debb95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Decision Tree Accuracy: 0.4134\n",
            "Custom Random Forest Accuracy: 0.7318\n",
            "Scikit-learn Linear SVM Accuracy: 0.7095\n",
            "Scikit-learn RBF SVM Accuracy (C=0.1): 0.6536\n",
            "Scikit-learn RBF SVM Accuracy (C=1): 0.6592\n",
            "Scikit-learn RBF SVM Accuracy (C=10): 0.7095\n"
          ]
        }
      ],
      "source": [
        "#Replace the accuracy variables with your defined variables for SVM accuracies\n",
        "\n",
        "print(f\"Custom Decision Tree Accuracy: {acc_tree:.4f}\")\n",
        "print(f\"Custom Random Forest Accuracy: {acc_forest:.4f}\")\n",
        "print(f\"Scikit-learn Linear SVM Accuracy: {acc_rbf:.4f}\")\n",
        "for c, acc in rbf_accuracies.items():\n",
        "    print(f\"Scikit-learn RBF SVM Accuracy (C={c}): {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n7lTe349_24"
      },
      "source": [
        "**Analytical Question:**\n",
        "1.  Looking at the RBF SVM results, how did changing the `C` parameter affect the model's accuracy? Based on the explanation above, what does your result suggest about the trade-off between a wide margin and fitting the training data for this problem?\n",
        "2.  Compared to your Random Forest, what is one advantage and one disadvantage of using an SVM? (Consider factors like interpretability, training time, and prediction performance)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJMei1mu9_25"
      },
      "source": [
        "**Answer:**\n",
        "1. As the C value increases, the accuracy also increases slightly — from 0.65 → 0.71.\n",
        "A low C (0.1) means SVM prioritizes a wider margin and allows some misclassifications. This prevents overfitting but can underfit complex data, leading to lower accuracy.\n",
        "A high C (10) means SVM enforces stricter classification of training points (tighter decision boundary, smaller margin), fitting the data more closely. This can reduce bias and improve accuracy on this dataset.\n",
        "\n",
        "2. Compared to the Random Forest, the SVM can create more complex and flexible decision boundaries between classes, which helps it handle complicated datasets.\n",
        "However, SVMs are harder to understand and take more time to train. The Random Forest gave better accuracy in this case and is easier to explain and interpret."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
