# -*- coding: utf-8 -*-
"""AI-Titanic Survival Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wRIBpToUyfvZfXlJqo63KNIA67y1DV6R

# Titanic Survival Prediction — Custom Decision Tree and Random Forest

**Roll Number:** *25280106*  
**Course:** AI500 — Machine Learning  
**Assignment:** Project 3 — Decision Trees & Random Forests  

---

### Overview

In this project, we build two models **from scratch** (without using scikit-learn classifiers):

1. **Part 1 — Custom Decision Tree**
2. **Part 2 — Custom Random Forest**
2. **Part 3 — SVM**

We train them on the **Titanic dataset** to predict passenger survival.  
We also analyze model accuracy and compare results.

**Import libraries**
"""

# ==============================
# TO BE IMPLEMENTED
# ==============================
import pandas as pd
from typing import Dict, Any
from sklearn.model_selection import train_test_split

from typing import Tuple, Optional, Union

# import are relvant libraries

"""### **Part 1 — Custom Decision Tree**

### Load and Prepare Titanic Dataset
"""

# Load training data
data = pd.read_csv("titanic.csv")

print(data.head())
print(data.info())
print(data.isnull().sum())

"""The Titanic dataset contains passenger information such as age, class, gender, fare, and survival status. Some columns contain missing values and categorical data that need to be handled properly.

---

## **Task: Implement Data Preprocessing Functions**

You are required to write Python functions (without using advanced preprocessing libraries like `sklearn.preprocessing`) to perform the following steps manually.

---

### *Step 1: Handle Missing Values*

Write functions to:

1. Replace missing values in the **Age** column with the **median age** of all passengers.  
2. Replace missing values in the **Embarked** column with the **most common embarkation port (mode)**.

---

### *Step 2: Drop Irrelevant Columns*

Remove the following columns that are not directly useful for survival prediction:

1. **Cabin** – too many missing values  
2. **Ticket** – non-numeric and not informative  
3. **Name** – mostly unique values  
4. **PassengerId** – identifier, not a feature

---

### **Step 3: Encode Categorical Variables**

Since machine learning models require numerical data, we need to convert categorical variables into numeric format manually.

#### Encode the `Sex` column:
- `male` → `0`  
- `female` → `1`

#### Encode the `Embarked` column:
- `C` → `0`  
- `Q` → `1`  
- `S` → `2`

This manual encoding ensures that categorical features can be interpreted correctly by machine learning algorithms, which typically work only with numerical inputs.

"""

# ==============================
# DATA PREPROCESSING
# ==============================

print("\n--- Step 1: Handle Missing Values by Imputation ---")

# 1. Impute Age with Median
median_age = data['Age'].median()
data['Age'].fillna(median_age, inplace=True)
print(f"Filled missing Age values with the median: {median_age:.2f}")

# 2. Impute Embarked with Mode
mode_embarked = data['Embarked'].mode().iloc[0]
data['Embarked'].fillna(mode_embarked, inplace=True)
print(f"Filled missing Embarked values with the mode: {mode_embarked}")


print("\n--- Step 2: Drop Irrelevant Columns ---")

# Columns you want to drop
columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']

data.drop(columns=columns_to_drop, axis=1, inplace=True)
print(f"Dropped columns: {columns_to_drop}")


print("\n--- Step 3: Encode Categorical Variables ---")

# Encode 'Sex'
sex_mapping = {'male': 0, 'female': 1}
data['Sex'] = data['Sex'].replace(sex_mapping)
print(f"Encoded 'Sex' column using mapping: {sex_mapping}")

# Encode 'Embarked'
embarked_mapping = {'C': 0, 'Q': 1, 'S': 2}
data['Embarked'] = data['Embarked'].replace(embarked_mapping)
print(f"Encoded 'Embarked' column using mapping: {embarked_mapping}")


print("\n--- Final Preprocessed Data (Head) ---")
print(data.head())

print(data.head())
print(data.info())
print(data.isnull().sum())

"""Split data into train and test (Do NOT change following cell)"""

X = data.drop('Survived', axis=1)
y = data['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## **Task: Implement Core Functions for Decision Tree Splitting**

In this section, you will implement the key mathematical components that allow a **Decision Tree** to decide where to split data during training. You will write three main functions: `entropy`, `information_gain`, and `best_split`.

---

### **Step 1: Compute Entropy**
- The entropy function measures the impurity or uncertainty in a dataset.  
- Formula:  
  $$
  H(y) = -\sum_i p_i \log_2(p_i)
  $$
  where $p_i$ is the probability of each class label.
- Use NumPy operations to calculate probabilities.
- Add a small constant (e.g., `1e-9`) inside the log to avoid numerical errors.
- Test on simple arrays, e.g., `entropy([0, 0, 1, 1])`.

---

### **Step 2: Compute Information Gain**
- Information Gain (IG) quantifies how much entropy decreases after a split.
- Formula:  
  $$
  IG = H(\text{parent}) - \frac{n_{left}}{n}H(\text{left}) - \frac{n_{right}}{n}H(\text{right})
  $$
- Implement this using the `entropy()` function.
- The higher the IG, the better the feature/threshold for splitting.

---

### **Step 3: Find the Best Split**
- The `best_split()` function finds which feature and threshold produce the **maximum information gain**.
- Loop through all features (or a given subset) and:
  - For **numerical features**:
    - Try splitting at all unique threshold values.
    - Create boolean masks for left (`<= t`) and right (`> t`) splits.
  - For **categorical features**:
    - Split data based on equality (`== val`) vs. inequality (`!= val`).
- Skip invalid splits (e.g., when one side is empty).
- Return:
  - `best_feature`: the most informative feature,
  - `best_threshold`: the split value,
  - `best_gain`: the highest information gain achieved.

---

### **Expected Output**
- `entropy()` returns 0 when all samples belong to one class.
- `information_gain()` returns higher values for better splits.
- `best_split()` identifies the optimal feature and threshold for partitioning the data.

Implement the functions below step by step.

"""

# ==========================
# TO BE IMPLEMENTED
# ==========================

import numpy as np
from typing import Tuple, Optional, Union, Dict, Any, List  #list and dict are used in the next cell

# A small constant to prevent log(0)
EPSILON = 1e-9

def entropy(y):
    """
    Compute the entropy of a label array y.
    Formula: H(y) = -Σ p_i * log2(p_i)
    """
    if len(y) == 0:
      return 0.0

    # Get the count of unique classes
    class_labels, counts = np.unique(y, return_counts=True)

    # Calculate probabilities p_i
    probabilities = counts / len(y)
    #Formula
    entropy_value = -np.sum(probabilities * np.log2(probabilities + EPSILON))

    return entropy_value


def information_gain(y, y_left, y_right):
    """
    Compute the information gain of a split.
    IG = H(parent) - (n_left/n)*H(left) - (n_right/n)*H(right)
    """
    n = len(y)
    n_left = len(y_left)
    n_right = len(y_right)

    if n_left == 0 or n_right == 0:
        # Avoid non-splits
        return 0.0

    # Calculate entropy for the parent node
    parent_entropy = entropy(y)

    # Calculate weighted entropy for the child nodes
    weighted_child_entropy = (n_left / n) * entropy(y_left) + \
                             (n_right / n) * entropy(y_right)

    # Calculate Information Gain
    ig = parent_entropy - weighted_child_entropy

    return ig


def best_split(X, y, feature_subset=None):
    """
    Find the best feature and threshold that maximize information gain.

    Parameters:
        X (DataFrame): Feature dataset
        y (Series or array): Target labels
        feature_subset (list): Optional subset of features for random forest

    Returns:
        best_feature (str): Feature name with best split
        best_threshold (float/str): Threshold or category value
        best_gain (float): Information gain of best split
    """
    best_gain = -1.0
    best_feature = None
    best_threshold = None

    features = feature_subset if feature_subset is not None else X.columns

    for feature_name in features:
        feature_data = X[feature_name]

        # Sort unique values
        unique_thresholds = np.sort(feature_data.unique())

        for threshold in unique_thresholds:
            # 1. Create split masks for numerical features
            mask_left = feature_data <= threshold
            mask_right = feature_data > threshold

            # Check if split is valid (both sides must have samples)
            if np.sum(mask_left) == 0 or np.sum(mask_right) == 0:
                continue

            # 2. Split the target array y
            y_left = y[mask_left]
            y_right = y[mask_right]

            # 3. Calculate Information Gain
            current_gain = information_gain(y, y_left, y_right)

            # 4. Update best split if current gain is higher
            if current_gain > best_gain:
                best_gain = current_gain
                best_feature = feature_name
                best_threshold = threshold

    return best_feature, best_threshold, best_gain

"""## **Task: Implement the Custom Decision Tree Classifier**

In this task, you will implement your own **Decision Tree** algorithm from scratch — without using libraries like `sklearn.tree`.  
This implementation will rely on the previously defined helper functions: `entropy`, `information_gain`, and `best_split`.

---

### **Class Overview**

You will create a `DecisionTree` class with the following methods:

---

### **1. `__init__()`**
- Initializes the tree’s hyperparameters:
  - `max_depth`: the maximum allowed depth of the tree.  
  - `min_samples_split`: minimum number of samples required to make a split.  
  - `feature_subsample`: number of randomly selected features (used in Random Forests).  
- The tree itself will be stored as a nested dictionary (`self.tree`).

---

### **2. `fit()`**
- Recursively builds the decision tree using **Information Gain** as the splitting criterion.
- **Base cases**:
  - If there are no samples left (`len(y) == 0`) → return `0`.
  - If all samples belong to the same class → return that class label.
  - If maximum depth is reached or too few samples remain → return the most common class (majority vote).
- **Recursive case**:
  1. Select the best feature and threshold using `best_split()`.
  2. Split the dataset into left and right subsets based on that threshold.
  3. Recursively call `fit()` on each subset to build subtrees.
- The final tree should be stored in `self.tree` as nested dictionaries with keys:
..., 'threshold': ..., 'gain': ..., 'left': ..., 'right': ...}


---

### **3. `predict_one()`**
- Predict the class for a **single data point**.
- Traverse the decision tree recursively:
- For numeric thresholds: use `<=` for the left branch, `>` for the right.
- For categorical values: use `==` for the left branch, `!=` for the right.
- Stop recursion when a leaf node (class label) is reached.

---

### **4. `predict()`**
- Predict the class for **an entire dataset** (`X`).
- Apply `predict_one()` to each row using a loop or list comprehension.
- Return predictions as a NumPy array.

---

### **Expected Behavior**
- The `fit()` method should build a decision tree automatically.
- The `predict()` method should output the correct class predictions for given input samples.
- You should be able to check accuracy by comparing predicted and actual labels.

---

### **Example Usage (after implementation)**

```python
tree = DecisionTree(max_depth=4)
tree.fit(X_train, y_train)
predictions = tree.predict(X_test)



"""

# ==============================
# TO BE IMPLEMENTED
# ==============================

import numpy as np
from collections import Counter
from sklearn.metrics import accuracy_score

class DecisionTree:
    def __init__(self, max_depth=5, min_samples_split=2, feature_subsample=None):
        """
        Initialize the Decision Tree parameters.
        """
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.feature_subsample = feature_subsample
        self.tree: Optional[Dict[str, Any]] = None

    def _majority_vote(self, y: np.ndarray) -> int:
        """Returns the most common class label in array y."""
        if len(y) == 0:
            return 0
        # Counter().most_common(1) returns [ (label, count) ]
        return Counter(y).most_common(1)[0][0]

    def _get_feature_subset(self, all_features: pd.Index) -> List[str]:
        """Selects a random subset of features."""
        if self.feature_subsample is None or self.feature_subsample >= len(all_features):
            return all_features.tolist()

        return np.random.choice(all_features, size=self.feature_subsample, replace=False).tolist()


    def fit(self, X, y, depth=0):
        """
        Recursively build the decision tree using information gain.
        """
        #Base Cases:

        # Base Case 1:
        if len(np.unique(y)) == 1:
            return y[0]

        # Base Case 2: Max Depth
        if depth >= self.max_depth or len(y) < self.min_samples_split:
            return self._majority_vote(y)

        # Ensure relevant columns are numeric before finding best split
        # This is a safeguard in case preprocessing wasn't fully applied or consistent
        X_processed = X.copy()
        for col in ['Age', 'Fare', 'Pclass', 'SibSp', 'Parch', 'Sex', 'Embarked']:
             if col in X_processed.columns:
                # Attempt conversion to numeric, coercing errors to NaN and then handling NaNs if any remain
                X_processed[col] = pd.to_numeric(X_processed[col], errors='coerce')


        # Recursive Case: Find Best Split
        features = self._get_feature_subset(X_processed.columns)
        feature, threshold, gain = best_split(X_processed, y, feature_subset=features)

        # Base Case 3:
        if feature is None or gain <= EPSILON:
            return self._majority_vote(y)

        # 1. Split Data
        # Use the processed data for splitting
        mask_left = X_processed[feature] <= threshold
        X_left, y_left = X_processed[mask_left], y[mask_left]
        X_right, y_right = X_processed[mask_left], y[mask_left]


        # 2. Recursively build subtrees
        left_subtree = self.fit(X_left, y_left, depth + 1)
        right_subtree = self.fit(X_right, y_right, depth + 1)

        # 3. Build Node Dictionary
        node = {
            'feature': feature,
            'threshold': threshold,
            'gain': gain,
            'left': left_subtree,
            'right': right_subtree
        }

        # Store the tree structure only at the root call
        if depth == 0:
            self.tree = node
            return self
        else:
            return node


    def predict_one(self, x, node=None):
        """
        Predict class label for a single sample.
        """
        if node is None:
            node = self.tree
            if node is None:
                raise ValueError("Tree not fitted. Call the function fit() first.")

        # If the node is not a dictionary, it's a leaf node (class label)
        if not isinstance(node, dict):
            return node

        feature = node['feature']
        threshold = node['threshold']

        # Add a safeguard to ensure the feature value is numeric for comparison if the threshold is numeric
        value = x[feature]
        if isinstance(threshold, (int, float)):
             # Attempt conversion to numeric, coercing errors to NaN
             value = pd.to_numeric(value, errors='coerce')
             # If conversion failed or resulted in NaN, we might need a strategy (e.g., default to one branch)
             # For now, let's assume valid numeric conversion after preprocessing
             if pd.isna(value):
                 # Handle cases where conversion to numeric failed after preprocessing
                 # This might indicate an issue with the data or preprocessing.
                 # For this context, let's arbitrarily send it down the right branch
                 return self.predict_one(x, node['right'])


        # Traverse the tree: Left (<= threshold) or Right (> threshold)
        # The comparison logic should align with how the split was determined in fit
        # Numerical split
        if isinstance(threshold, (int, float)):
            if value <= threshold:
                return self.predict_one(x, node['left'])
            else:
                return self.predict_one(x, node['right'])
        # Categorical split (equality-based)
        else:
             if value == threshold:
                 return self.predict_one(x, node['left'])
             else:
                 return self.predict_one(x, node['right'])


    def predict(self, X):
        """
        Predict class labels for all rows in dataset X.
        """
        if self.tree is None:
            # Check if the tree was actually fitted
            raise ValueError("Tree not fitted. Call fit() first.")

        # Apply predict_one to each row of the input DataFrame X
        predictions = np.array([self.predict_one(X.iloc[i]) for i in range(len(X))])
        return predictions

"""### **Train and Evaluate the Custom Decision Tree**

Do **not** change the following cell.  
It will be used to train, test, and evaluate your implemented `DecisionTree` class automatically.

"""

tree = DecisionTree(max_depth=4)
tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)

acc_tree = np.mean(y_pred_tree == y_test)
print(f"Custom Decision Tree Accuracy: {acc_tree:.4f}")

compare_tree = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_tree})
compare_tree.head(10)

"""## **Part 2 — Custom Random Forest**

In this section, you will implement your own **Random Forest Classifier** from scratch — without using libraries like `sklearn.ensemble`.

A Random Forest combines multiple Decision Trees to form an ensemble model that improves prediction accuracy and reduces overfitting.

---

### **Instructions**

1. **Initialize Parameters (`__init__`)**
   - `n_estimators`: Number of decision trees to train.
   - `max_depth`: Maximum depth of each decision tree.
   - `min_samples_split`: Minimum number of samples required to split a node.
   - `feature_subsample_ratio`: Fraction of features to randomly select for each tree (e.g., 0.7 = 70% of features per tree).

   Store all trees in a list called `self.trees`.

---

2. **Train the Model (`fit`)**
   - For each tree:
     - Create a **bootstrap sample** by randomly selecting rows from the dataset **with replacement**.
     - Randomly choose a subset of features using the ratio `feature_subsample_ratio`.
     - Train a new instance of your `DecisionTree` class on the sampled data.
   - Append each trained tree to the list `self.trees`.

---

3. **Make Predictions (`predict`)**
   - Each trained decision tree makes predictions for all samples in `X`.
   - Combine predictions from all trees using **majority voting** (most common class).
   - Return the final predicted labels as a NumPy array.

---

### **Expected Behavior**
- The Random Forest should generally achieve **equal or better accuracy** than your single Decision Tree.
- The `fit()` and `predict()` methods should work seamlessly with your previously defined `DecisionTree` implementation.

---

### **Goal**
By completing this part, you will understand how ensemble learning improves model robustness through randomness and aggregation.

Implement Custom Random Forest Class
"""

# ==============================
# TO BE IMPLEMENTED
# ==============================

import numpy as np
from collections import Counter

class RandomForest:
    def __init__(self, n_estimators=10, max_depth=5, min_samples_split=2, feature_subsample_ratio=0.7):
        """
        Initialize the Random Forest parameters.
        """
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.feature_subsample_ratio = feature_subsample_ratio
        self.trees: List[DecisionTree] = []


    def fit(self, X, y):
        """
        Train multiple Decision Trees using bootstrapped samples and random feature subsets.
        """
        n_samples = len(X)
        n_features = len(X.columns)

        # Calculate the actual number of features to subsample
        feature_subsample_count = int(n_features * self.feature_subsample_ratio)

        self.trees = []

        for _ in range(self.n_estimators):
            # 1. Create a bootstrap sample (sampling rows with replacement)
            bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)
            # Use iloc and .copy() for reliable sampling and type conversion prevention
            X_sample = X.iloc[bootstrap_indices].copy()
            y_sample = y[bootstrap_indices]

            # 2. Train a new DecisionTree instance
            tree = DecisionTree(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                feature_subsample=feature_subsample_count
            )

            tree.fit(X_sample, y_sample)

            # 3. Append the trained tree
            self.trees.append(tree)


    def predict(self, X):
        """
        Predict class labels for all samples using majority voting from all trained trees.
        """
        if not self.trees:
            raise ValueError("Random Forest not fitted. Call fit() first.")

        # Get predictions from all individual trees (Shape: n_estimators x n_samples)
        predictions_all = np.array([tree.predict(X) for tree in self.trees])

        # Combine predictions using majority voting across all trees for each sample
        final_predictions = np.array([
            Counter(predictions_all[:, i]).most_common(1)[0][0]
            for i in range(predictions_all.shape[1])
        ])

        return final_predictions

"""### **Train and Evaluate the Custom Random Forest**

Do **not** change the following cell.  
It will be used to train, test, and evaluate your implemented `RandomForest` class automatically.

"""

forest = RandomForest(n_estimators=10, max_depth=5)
forest.fit(X_train, y_train)
y_pred_forest = forest.predict(X_test)

acc_forest = np.mean(y_pred_forest == y_test)
print(f"Custom Random Forest Accuracy: {acc_forest:.4f}")

compare_forest = pd.DataFrame({
    'Actual': y_test,
    'Tree_Pred': y_pred_tree,
    'Forest_Pred': y_pred_forest
})
compare_forest.head(7)

"""### **Comparison**

Do **not** change the following cell.  
It compares the performance of your **Custom Decision Tree** and **Custom Random Forest** implementations by printing their respective accuracies.

"""

print(f"Decision Tree Accuracy: {acc_tree:.4f}")
print(f"Random Forest Accuracy: {acc_forest:.4f}")

"""**Analytical Question:**  
Based on the accuracy results of the Decision Tree and Random Forest models, explain **why the Random Forest might perform better or worse** than a single Decision Tree.  
Discuss the roles of **ensemble learning**, **variance reduction**, and **overfitting** in your explanation.

**Answer:**
Random Forest performed better than a single Decision Tree.This large improvement is due to the power of ensemble learning and variance reduction(high to low variance).

A Random Forest combines predictions from many independent n Decision Trees (each trained on different n subsets of data and features).
This making of multiple Decision Trees called ensemble learning which leverages the idea that while each tree may be imperfect, their combined “vote” tends to be much more accurate(Majority wins voting)

A single Decision Tree is a high-variance model, a small changes in the training data can lead to very different outcomes.
By using multiple trees (each trained on a random sample), Random Forest reduces variance without significantly increasing bias.

A single Decision Tree tends to overfit training data because it grows deep and memorizes patterns specific to the dataset.
Random Forest prevents overfitting by Bootstrap sampling (each tree sees a different subset of the data). The second one is random feature sampling, each tree uses a random subset of features at each split.

---
## **Part 3 — Support Vector Machines (SVM) with Scikit-Learn**

In this final part, you will explore another powerful supervised learning algorithm: the **Support Vector Machine (SVM)**. Unlike Decision Trees, which make splits by partitioning data, an SVM's goal is to find the optimal hyperplane or decision boundary that best separates the classes.

You will use `scikit-learn`'s implementation to train an SVM on the same preprocessed Titanic data and compare its performance to your from-scratch models.

### **Task: Train and Evaluate SVM Classifiers**

You will use `scikit-learn`'s `SVC` (Support Vector Classifier). We will experiment with two key hyperparameters: the `kernel` and the regularization parameter `C`.

#### **Understanding SVM Kernels**
The **kernel** allows the SVM to find complex, non-linear decision boundaries. We will use two types:
* **`linear` kernel:** This attempts to separate the data with a single straight line (or a flat plane in higher dimensions). It's fast and works well if the data is linearly separable. Think of it as using a ruler to divide two groups of points.
* **`rbf` kernel (Radial Basis Function):** This is the default and more flexible kernel. It can create complex, curved decision boundaries by mapping the data to a higher-dimensional space. Think of it as using a flexible wire to enclose different groups of points.

#### **Understanding the Regularization Parameter `C`**
The **`C` parameter** controls the trade-off between achieving a low training error and a low testing error.
* A **low `C`** makes the decision boundary smoother and the margin wider. The model is more tolerant of misclassifying a few training points, which can lead to better **generalization** (less overfitting).
* A **high `C`** aims to classify every training example correctly, which can lead to a more complex boundary and a narrower margin. This might **overfit** the training data.

**Instructions:**
1.  Use the same `X_train`, `y_train`, `X_test`, and `y_test` data from Part 1.
2.  Train one `SVC` model with a `linear` kernel as a baseline.
3.  Train three separate `SVC` models with an `rbf` kernel, using `C` values of `0.1`, `1`, and `10`.
4.  For each of the four models, calculate and print its accuracy on the test set.
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

print("\n==============================")
print(" PART 3 — Support Vector Machines ")
print("==============================\n")

# ------------------------------------------
# 1. Linear Kernel SVM
# ------------------------------------------
print("Training SVM with LINEAR kernel...")
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)

y_pred_linear = svm_linear.predict(X_test)
acc_linear = accuracy_score(y_test, y_pred_linear)

print(f"Accuracy (Linear Kernel): {acc_linear:.4f}\n")


# ------------------------------------------
# 2. RBF Kernel SVM (C = 0.1)
# ------------------------------------------
print("Training SVM with RBF kernel, C = 0.1...")
svm_rbf_c01 = SVC(kernel='rbf', C=0.1)
svm_rbf_c01.fit(X_train, y_train)

y_pred_rbf_c01 = svm_rbf_c01.predict(X_test)
acc_rbf_c01 = accuracy_score(y_test, y_pred_rbf_c01)

print(f"Accuracy (RBF, C=0.1): {acc_rbf_c01:.4f}\n")


# ------------------------------------------
# 3. RBF Kernel SVM (C = 1)
# ------------------------------------------
print("Training SVM with RBF kernel, C = 1...")
svm_rbf_c1 = SVC(kernel='rbf', C=1)
svm_rbf_c1.fit(X_train, y_train)

y_pred_rbf_c1 = svm_rbf_c1.predict(X_test)
acc_rbf_c1 = accuracy_score(y_test, y_pred_rbf_c1)

print(f"Accuracy (RBF, C=1): {acc_rbf_c1:.4f}\n")


# ------------------------------------------
# 4. RBF Kernel SVM (C = 10)
# ------------------------------------------
print("Training SVM with RBF kernel, C = 10...")
svm_rbf_c10 = SVC(kernel='rbf', C=10)
svm_rbf_c10.fit(X_train, y_train)

y_pred_rbf_c10 = svm_rbf_c10.predict(X_test)
acc_rbf_c10 = accuracy_score(y_test, y_pred_rbf_c10)

print(f"Accuracy (RBF, C=10): {acc_rbf_c10:.4f}\n")

"""### **Final Comparison and Analysis**

Now, let's compare the performance of all the models you've worked with.
"""

#Replace the accuracy variables with your defined variables for SVM accuracies



print("\n========== SUMMARY ==========")
print(f"Custom Decision Tree Accuracy: {acc_tree:.4f}")
print(f"Custom Random Forest Accuracy: {acc_forest:.4f}")

print(f"Training SVM with LINEAR kernel: {acc_linear:.4f}\n")
print(f"Scikit-learn Linear SVM Accuracy, C=0.1      : {acc_rbf_c01:.4f}")
print(f"Scikit-learn Linear SVM Accuracy, C=1        : {acc_rbf_c1:.4f}")
print(f"Scikit-learn Linear SVM Accuracy, C=10       : {acc_rbf_c10:.4f}")
print("=============================\n")

**Questions for understanding**
  
1. Looking at the Linear SVM results, how did changing the C parameter affect the model’s accuracy? Based on the explanation above, what does this suggest about the trade-off between margin width and fitting the training data?

As the C value increased, the SVM's accuracy also improved:

- C = 0.1 → Accuracy = 0.6536
- C = 1 → Accuracy = 0.6592
- C = 10 → Accuracy = 0.7095

A low C (0.1) prefers a wider margin and allows more misclassifications.
This makes the model simpler and prevents overfitting, but in this case it underfits, resulting in lower accuracy.
A higher C (10) forces the model to classify more training samples correctly by using a tighter margin.
This increases the model’s complexity and reduces bias, which improved accuracy on this dataset.
Your results suggest that the Titanic dataset benefits from a higher-C SVM, meaning a stricter, more detailed decision boundary works better than a wide-margin one.

2. Compared to your Random Forest (accuracy = 0.6257), what is one advantage and one disadvantage of using SVM?
**Advantage of SVM:**
The SVM achieved higher accuracy (up to 0.7095 with C=10) than the Random Forest (0.6257).
This shows SVM can capture more subtle patterns in the Titanic dataset when tuned properly.

**Disadvantage of SVM:**
SVM models—especially with different C values—are harder to interpret and often slower to train, especially on larger datasets.
Random Forests are easier to understand (feature importance, tree structure) and generally faster to train at scale.
